{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from saicinpainting.training.modules.ffc import FFCResNetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loc = Path(\"checkpoints/big-lama\")\n",
    "\n",
    "ckpt = torch.load(model_loc.joinpath(\"models/best.ckpt\"), map_location=\"cpu\")\n",
    "gen_weight = {k.replace(\"generator.\", \"\"):v for k,v in ckpt[\"state_dict\"].items() if k.startswith(\"generator.\")}\n",
    "hparams = yaml.load(model_loc.joinpath(\"config.yaml\").read_bytes())[\"generator\"]\n",
    "del hparams[\"kind\"]\n",
    "\n",
    "model = FFCResNetGenerator(**hparams).eval().to(\"cuda:1\")\n",
    "model.load_state_dict(gen_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = [nn.ReflectionPad2d(3), FFC_BN_ACT_local(4, 64, 7, 1, 0)]\n",
    "\n",
    "### downsample\n",
    "model += [\n",
    "    FFC_BN_ACT_local(64, 128, 3, 2, 1), \n",
    "    FFC_BN_ACT_local(128, 256, 3, 2, 1),\n",
    "    FFC_BN_ACT_local2global(256, 512, 3, 2, 1, ratio_gout=0.75)\n",
    "]\n",
    "\n",
    "mult = 8\n",
    "feats_num_bottleneck = 512\n",
    "\n",
    "### resnet blocks\n",
    "for i in range(18):\n",
    "    model += [FFCResnetBlock(512, 0.75, enable_lfu=False)]\n",
    "\n",
    "model += [ConcatTupleLayer()]\n",
    "\n",
    "### upsample\n",
    "for i in range(3):\n",
    "    mult = 2 ** (3 - i)\n",
    "    model += [nn.ConvTranspose2d(min(1024, 64 * mult),  min(1024, int(64 * mult / 2)),\n",
    "                                    kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.BatchNorm2d(min(1024, int(64 * mult / 2))),\n",
    "                nn.ReLU(inplace=True)]\n",
    "\n",
    "model += [nn.ReflectionPad2d(3), nn.Conv2d(64, 3, kernel_size=7, padding=0)]\n",
    "model.append(nn.Sigmoid())\n",
    "\n",
    "model = nn.Sequential(*model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "\n",
    "img_data = Image.open(\"/datasets/RD/interactive-LAMA/img/webui/21-12-31_04:47:28:876/001-000-2a5ce1afb166.jpg\")\n",
    "# mask = img_data.point(lambda p: p <= 0 and 255)\n",
    "mask = Image.new('L', img_data.size)\n",
    "d = img_data.getdata()\n",
    "new_d = []\n",
    "for item in d:\n",
    "    if item[0] in range(240,256) and item[1] in range(0,16) and item[2] in range(240,256):\n",
    "        new_d.append(255)\n",
    "    else:\n",
    "        new_d.append(0)\n",
    "mask.putdata(new_d)\n",
    "\n",
    "mask = mask.resize((img_data.width, img_data.height), 3)\n",
    "mask = mask.filter(ImageFilter.ModeFilter(size=13))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img  = cv2.imread(\"../interactive-LAMA/img/tigerbro_clean_v2/001-000-2a5ce1afb166.jpg\")\n",
    "img  = (img  / 255.).astype(np.float32)\n",
    "\n",
    "#mask = cv2.imread(\"/datasets/RD/interactive-LAMA/img/webui/21-12-31_03:15:43:310/001-000-2a5ce1afb166_mask.jpg\")[:,:,0:1]\n",
    "# mask = cv2.resize(np.asarray(mask), img.shape[:2][::-1])\n",
    "mask = mask.resize((img.shape[1], img.shape[0]), 3)\n",
    "mask = np.expand_dims(np.asarray(mask), axis=-1) \n",
    "mask = ((mask / 255.) > 0.9).astype(np.float32)\n",
    "\n",
    "h, w, c = img.shape\n",
    "out_h = h if h % 8 == 0 else (h // 8 + 1) * 8\n",
    "out_w = w if w % 8 == 0 else (w // 8 + 1) * 8\n",
    "img_t  = np.pad(img,  ((0, out_h-h), (0, out_w-w), (0,0)), mode='symmetric')\n",
    "mask_t = np.pad(mask, ((0, out_h-h), (0, out_w-w), (0,0)), mode='symmetric')\n",
    "\n",
    "img_t = torch.from_numpy(img_t).permute(2,0,1).to(\"cuda:0\")\n",
    "mask_t = torch.from_numpy(mask_t).permute(2,0,1).to(\"cuda:0\")\n",
    "masked_img_t = img_t * (1 - mask_t)\n",
    "masked_img_t = torch.cat([masked_img_t, mask_t], 0).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_image = model(masked_img_t)\n",
    "\n",
    "inpaint = mask_t * predicted_image + (1 - mask_t) * img_t\n",
    "predict = inpaint[0].permute(1,2,0).cpu().numpy()\n",
    "#predict = (predict * 255.).astype(np.uint8)\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(24,8))\n",
    "ax[0].imshow(img[:,:,::-1])\n",
    "ax[1].imshow(mask[:,:,0])\n",
    "ax[2].imshow(predict[:,:,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
